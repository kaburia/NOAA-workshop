{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "43eda2f0",
      "metadata": {
        "id": "43eda2f0"
      },
      "source": [
        "# Focus Area 3 ‚Äî Temperature Quality &amp; Microclimates\n",
        "**Core Objective**: To demonstrate the advantages of high-resolution temperature data in\n",
        "capturing microclimates and computing derived metrics like PET, for better assessment of heat-\n",
        "related risks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa97e7ce",
      "metadata": {
        "id": "fa97e7ce"
      },
      "source": [
        "## Extract Temperature data\n",
        "- GHCN data\n",
        "- CBAM data\n",
        "- ERA5 data\n",
        "<br>\n",
        "\n",
        "EA: March 2025 heatwave, linking to health impacts.\n",
        "\n",
        "Use the current location and get the nearest GHCNd weather station and visualise the temperature over the last half a century"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6df409a",
      "metadata": {
        "id": "f6df409a"
      },
      "source": [
        "Require 2 files\n",
        "- The Metadata file: TAHMO_Metadata.csv\n",
        "- The TAHMO data file: TAHMO_data.csv\n",
        "\n",
        "Metadata file format (Columns):\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>TAHMO Metadata</title>\n",
        "</head>\n",
        "<body>\n",
        "    <table border=\"1\">\n",
        "        <tr>\n",
        "            <th>Code</th>\n",
        "            <th>lat</th>\n",
        "            <th>lon</th>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>TA00283</td>\n",
        "            <td>1.2345</td>\n",
        "            <td>36.7890</td>\n",
        "        </tr>\n",
        "        <!-- More rows as needed -->\n",
        "    </table>\n",
        "</html>\n",
        "\n",
        "Data file format (Columns): Temperature / Precipitation data for multiple stations\n",
        "<html>\n",
        "<head>\n",
        "    <title>TAHMO Data</title>\n",
        "</head>\n",
        "<body>\n",
        "    <table border=\"1\">\n",
        "        <tr>\n",
        "            <th>Date</th>\n",
        "            <th>TA00283</th>\n",
        "            <th>TA00284</th>\n",
        "            <th>TA00285</th>\n",
        "            <!-- More station codes as needed -->\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>2023-01-01</td>\n",
        "            <td>25.3</td>\n",
        "            <td>26.1</td>\n",
        "            <td>24.8</td>\n",
        "        </tr>\n",
        "        <!-- More rows as needed -->\n",
        "    </table>\n",
        "</html>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps Breakdown\n",
        "- Step 1: Setting up environment and Authentication\n",
        "\n",
        "- Data Extraction and Loading\n",
        "    - Step2: Search and Select Region of Interest please use the same region as you did in the previous focus area\n",
        "\n",
        "- Data Processing and QC\n",
        "\n",
        "    - Step 6:"
      ],
      "metadata": {
        "id": "NpbU68UkNNRY"
      },
      "id": "NpbU68UkNNRY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XWw3IATm-_J9",
      "metadata": {
        "id": "XWw3IATm-_J9",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Step 1a: Setting up environment installing required Dependencies\n",
        "# @markdown This cell installs the required dependencies for the workshop. It may take a few minutes <br>\n",
        "# @markdown If you encounter any errors, please restart the runtime and try again. <br>\n",
        "# @markdown If the error persists, please seek help.\n",
        "\n",
        "\n",
        "print(\"Installing required dependencies...\")\n",
        "!pip install git+https://github.com/kaburia/NOAA-workshop.git > /dev/null 2>&1\n",
        "\n",
        "!jupyter nbextension enable --py widgetsnbextension\n",
        "\n",
        "# check there was no error\n",
        "import sys\n",
        "if not sys.argv[0].endswith(\"kernel_launcher.py\"):\n",
        "    print(\"‚ùå Errors occurred during installation. Please restart the runtime and try again.\")\n",
        "else:\n",
        "    print(\"‚úÖ Dependencies installed successfully.\")\n",
        "\n",
        "print(\"Importing required libraries...\")\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import ee\n",
        "import xarray as xr\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr, ttest_rel\n",
        "\n",
        "# import os\n",
        "# os.chdir('NOAA-workshop')\n",
        "\n",
        "from utils.ground_stations import plot_stations_folium\n",
        "from utils.helpers import get_region_geojson\n",
        "from utils.CHIRPS_helpers import get_chirps_pentad_gee\n",
        "from utils.CBAM_helpers import CBAMClient, extract_cbam_data # CBAM helper functions\n",
        "from utils.plotting import select, scale, plot_xarray_data, plot_xarray_data2, compare_xarray_datasets, compare_xarray_datasets2 # Plotting helper functionsfrom utils.IMERG_helpers import get_imerg_raw\n",
        "from utils.ERA5_helpers import era5_data_extracts, era5_var_handling\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "import pandas as pd\n",
        "import json\n",
        "import ee\n",
        "from scipy.stats import pearsonr\n",
        "import seaborn as sns\n",
        "from utils.filter_stations import RetrieveData\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully.\")\n",
        "\n",
        "def build_xr_from_stations(ds, stations_metadata, var_name=None):\n",
        "    # Auto-detect variable if not provided\n",
        "    if var_name is None:\n",
        "        candidate_vars = ['total_precipitation', 'total_rainfall', 'precipitation']\n",
        "        found = [v for v in candidate_vars if v in ds.data_vars]\n",
        "        if not found:\n",
        "            raise ValueError(f\"None of expected precipitation variable names {candidate_vars} found in dataset vars: {list(ds.data_vars)}\")\n",
        "        var_name = found[0]\n",
        "\n",
        "    # Determine dimension names\n",
        "    if {'x', 'y'}.issubset(ds.dims):\n",
        "        lon_dim, lat_dim = 'x', 'y'\n",
        "    elif {'lon', 'lat'}.issubset(ds.dims):\n",
        "        lon_dim, lat_dim = 'lon', 'lat'\n",
        "    else:\n",
        "        raise ValueError(f\"Dataset dims {list(ds.dims)} do not contain expected (x,y) or (lon,lat).\")\n",
        "\n",
        "    all_stations_data = {}\n",
        "    for _, row in stations_metadata.iterrows():\n",
        "        station_code = row['code']\n",
        "        lat = float(row['lat'])\n",
        "        lon = float(row['lon'])\n",
        "        # Skip stations outside domain (quick bounds check)\n",
        "        if not (ds[lon_dim].min() <= lon <= ds[lon_dim].max() and ds[lat_dim].min() <= lat <= ds[lat_dim].max()):\n",
        "            continue\n",
        "        station_da = ds[var_name].sel({lon_dim: lon, lat_dim: lat}, method=\"nearest\")\n",
        "        station_df = station_da.to_dataframe(name=station_code)\n",
        "        all_stations_data[station_code] = station_df[station_code]\n",
        "\n",
        "    combined_df = pd.DataFrame(all_stations_data)\n",
        "    return combined_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HXnAaHsG_cWI",
      "metadata": {
        "id": "HXnAaHsG_cWI",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title ### Step 1b: Authentication Step\n",
        "# @markdown This step is used to authenticate you as a user and there will be two popups that will be doing this.\n",
        "# @markdown 1. **Authentication to Google Drive** - This is where we shall be loading the data after we have extracted it\n",
        "# @markdown 2. **Authentication to Google Earth Engine** - This will be used to extract the CHIRPS data and any other satellite product we shall be extracting in the future.\n",
        "# @markdown Please check your email we shared an invitation to our Google Cloud Project that we shall be using to extract the data\n",
        "# @markdown *PS: In the future, to create your own project, please refer to [Google Cloud's Documentation](https://developers.google.com/earth-engine/guides/access) that shows the step by step breakdown of creating a Google Cloud Project and enabling Google Earth Engine*\n",
        "# @markdown  Link to configure noncommercial use of Google Earth Engine: https://console.cloud.google.com/earth-engine/configuration\n",
        "\n",
        "print(\"Authenticating to Google Drive...\")\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "print(\"‚úÖ Google Drive authenticated successfully.\")\n",
        "\n",
        "print(\"Authenticating to Google Earth Engine...\")\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project='noaa-tahmo')\n",
        "print(\"‚úÖ Google Earth Engine authenticated successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A config file is provided with the api keys to access TAHMO Data\n",
        "```json\n",
        "{\n",
        "    \"apiKey\": \"\",\n",
        "    \"apiSecret\": \"\",\n",
        "    \"location_keys\": \"\",\n",
        "    \"cbam_username\" : \"\",\n",
        "    \"cbam_password\" : \"\"\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "Z0C8VQhrM-Ul"
      },
      "id": "Z0C8VQhrM-Ul"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 1c: Please upload the provided config file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dXJBF3FUM4B4"
      },
      "id": "dXJBF3FUM4B4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2: Search and Select Region of Interest please use the same region as you did in the previous focus area\n",
        "# @markdown Enter the name of a country, region, or place in Africa. <br>\n",
        "# @markdown The Google Maps API will be used to fetch its geometry, bounding box, and show the polygon on a map.<br>\n",
        "# @markdown We highly recommend to use a region within the East Africa for everything to run smoothly\n",
        "\n",
        "import json\n",
        "import folium\n",
        "\n",
        "# print(\"Loading config file...\")\n",
        "# Load config file\n",
        "# Load API key\n",
        "with open('/content/config.json', 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "location_key = config['location_keys']  # Google Maps API key\n",
        "\n",
        "# --- Helper functions ---\n",
        "def xmin_ymin_xmax_ymax(polygon):\n",
        "    lons = [pt[0] for pt in polygon]\n",
        "    lats = [pt[1] for pt in polygon]\n",
        "    return min(lons), min(lats), max(lons), max(lats)\n",
        "\n",
        "def fetch_region(query):\n",
        "    region_geom = get_region_geojson(query, location_key)['geometry']['coordinates'][0]\n",
        "    xmin, ymin, xmax, ymax = xmin_ymin_xmax_ymax(region_geom)\n",
        "    print(f\"‚úÖ Selected: {query}\")\n",
        "    print(f\"Bounding box -> xmin: {xmin}, ymin: {ymin}, xmax: {xmax}, ymax: {ymax}\")\n",
        "    return region_geom, (xmin, ymin, xmax, ymax)\n",
        "\n",
        "def show_region_polygon(polygon):\n",
        "    # Center map on polygon\n",
        "    lons = [pt[0] for pt in polygon]\n",
        "    lats = [pt[1] for pt in polygon]\n",
        "    m = folium.Map(location=[sum(lats)/len(lats), sum(lons)/len(lons)], zoom_start=6)\n",
        "    folium.Polygon(\n",
        "        locations=[(lat, lon) for lon, lat in polygon],\n",
        "        color=\"blue\",\n",
        "        weight=2,\n",
        "        fill=True,\n",
        "        fill_opacity=0.3,\n",
        "    ).add_to(m)\n",
        "    return m\n",
        "\n",
        "# --- Main logic ---\n",
        "region_query = input(\"üåç Enter the name of a region (e.g. Kenya, Uganda, Serengeti): \")\n",
        "\n",
        "try:\n",
        "    region_geom, bbox = fetch_region(region_query)\n",
        "    xmin, ymin, xmax, ymax = xmin_ymin_xmax_ymax(region_geom)\n",
        "    m = show_region_polygon(region_geom)  # draw polygon, not bbox\n",
        "    display(m)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Could not fetch geometry for '{region_query}': {e}\")\n",
        "    print(\"üîÅ Please re-run the cell and enter a different region.\")\n",
        "\n",
        "# .lower\n",
        "region_query = region_query.lower()\n",
        "\n",
        "start_date = \"2025-04-01\"\n",
        "end_date = \"2025-05-31\"\n",
        "\n",
        "dir_path = '/content/drive/MyDrive/NOAA-workshop-data'\n",
        "os.makedirs(dir_path, exist_ok=True)\n",
        "# check if the path was created successfully\n",
        "if not os.path.exists(dir_path):\n",
        "    print(\"‚ùå Path not created successfully.\")\n",
        "else:\n",
        "    print(\"‚úÖ Path created successfully.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7JvxHhHtNEQ7"
      },
      "id": "7JvxHhHtNEQ7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  @title Dependencies point plotting Functions\n",
        "\n",
        "import random\n",
        "\n",
        "def plot_temperatures(tmin_df, tavg_df, tmax_df, station_code=None):\n",
        "    \"\"\"\n",
        "    Plots the daily minimum, average, and maximum temperatures for a specified TAHMO station.\n",
        "\n",
        "    Args:\n",
        "        tmin_df (pd.DataFrame): DataFrame containing daily minimum temperatures.\n",
        "        tavg_df (pd.DataFrame): DataFrame containing daily average temperatures.\n",
        "        tmax_df (pd.DataFrame): DataFrame containing daily maximum temperatures.\n",
        "        station_code (str, optional): The code of the station to plot. If None, a random station from the DataFrame is plotted.\n",
        "    \"\"\"\n",
        "    if station_code is None:\n",
        "        station_code = random.choice(tmin_df.columns.tolist())\n",
        "        print(f\"Randomly selected station: {station_code}\")\n",
        "    elif station_code not in tmin_df.columns:\n",
        "        print(f\"Station code {station_code} not found in the data.\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(tmin_df.index, tmin_df[station_code], label='Min Temp', linestyle='-')\n",
        "    plt.plot(tavg_df.index, tavg_df[station_code], label='Avg Temp', linestyle='-')\n",
        "    plt.plot(tmax_df.index, tmax_df[station_code], label='Max Temp', linestyle='-')\n",
        "\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Temperature (¬∞C)')\n",
        "    plt.title(f'Daily Temperatures for Station {station_code}')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xuDz7AdEGfdX"
      },
      "id": "xuDz7AdEGfdX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ERA5 builder\n",
        "import ee\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.colors\n",
        "import math\n",
        "import datetime\n",
        "import io\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime, timedelta\n",
        "from IPython.display import HTML, display\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "from filter_stations import retreive_data, Filter\n",
        "import base64\n",
        "import json\n",
        "import requests\n",
        "import datetime\n",
        "from utils.helpers import get_region_geojson, df_to_xarray\n",
        "\n",
        "\n",
        "\n",
        "def extract_era5_daily(start_date_str, end_date_str, bbox=None, polygon=None, era5_l=False, aggregate='mean'):\n",
        "    \"\"\"\n",
        "    Extract ERA5 reanalysis data (daily aggregated) from Google Earth Engine for a given bounding box or polygon and time range.\n",
        "    The extraction is performed on a daily basis by aggregating hourly images (using the mean) for each day.\n",
        "    For each day, the function retrieves the ERA5 HOURLY images, aggregates them, adds pixel coordinate bands (longitude\n",
        "    and latitude), and uses sampleRectangle to extract a grid of pixel values. The results for each variable (band) are then\n",
        "    organized into pandas DataFrames with the following columns:\n",
        "      - date: The daily timestamp (ISO formatted)\n",
        "      - latitude: The latitude coordinate of the pixel center\n",
        "      - longitude: The longitude coordinate of the pixel center\n",
        "      - value: The aggregated pixel value for that variable\n",
        "\n",
        "    Args:\n",
        "        start_date_str (str): Start datetime in ISO format, e.g., '2020-01-01T00:00:00'.\n",
        "        end_date_str (str): End datetime in ISO format, e.g., '2020-01-02T00:00:00'.\n",
        "        bbox (list or tuple, optional): Bounding box specified as [minLon, minLat, maxLon, maxLat]. Default is None.\n",
        "        polygon (list, optional): Polygon specified as a list of coordinate pairs (e.g., [[lon, lat], ...]).\n",
        "                                  If provided, the polygon geometry will be used instead of the bounding box.\n",
        "                                  Default is None.\n",
        "        era5_l (bool, optional): If True, use ERA5_LAND instead of ERA5. Default is False.\n",
        "        aggregate (str, optional): Aggregation method ('mean' or 'sum' or 'min', or 'max'). Default is 'mean'.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are variable (band) names and values are pandas DataFrames containing\n",
        "              the daily aggregated data.\n",
        "    \"\"\"\n",
        "    # Convert input datetime strings to Python datetime objects.\n",
        "    start_date = datetime.datetime.strptime(start_date_str, '%Y-%m-%dT%H:%M:%S')\n",
        "    end_date   = datetime.datetime.strptime(end_date_str, '%Y-%m-%dT%H:%M:%S')\n",
        "\n",
        "    # Define the geometry: Use polygon if provided, otherwise use bbox.\n",
        "    if polygon is not None:\n",
        "        region = ee.Geometry.Polygon(polygon)\n",
        "    elif bbox is not None:\n",
        "        region = ee.Geometry.Rectangle(bbox)\n",
        "    else:\n",
        "        raise ValueError(\"Either bbox or polygon must be provided.\")\n",
        "\n",
        "    # Define a scale in meters corresponding approximately to 0.25¬∞ (at the equator, 1¬∞ ‚âà 111320 m).\n",
        "    scale_m = 27830\n",
        "\n",
        "    # This dictionary will accumulate extracted records for each variable (band).\n",
        "    results = {}\n",
        "\n",
        "    # Loop over each day in the specified time range.\n",
        "    current = start_date\n",
        "    while current < end_date:\n",
        "        next_day = current + datetime.timedelta(days=1)\n",
        "\n",
        "        # Format the current time window in ISO format.\n",
        "        t0_str = current.strftime('%Y-%m-%dT%H:%M:%S')\n",
        "        t1_str = next_day.strftime('%Y-%m-%dT%H:%M:%S')\n",
        "\n",
        "        print(f\"Processing {t0_str} to {t1_str}\")\n",
        "\n",
        "        # If ER5 Land (0.1) or ERA5 (0.25)\n",
        "        if era5_l:\n",
        "            # Get the ERA5 Land hourly image collection for the current day.\n",
        "            collection = ee.ImageCollection('ECMWF/ERA5_LAND/HOURLY') \\\n",
        "                            .filterDate(ee.Date(t0_str), ee.Date(t1_str))\n",
        "        else:\n",
        "            # Get the ERA5 hourly image collection for the current day.\n",
        "            collection = ee.ImageCollection('ECMWF/ERA5/HOURLY') \\\n",
        "                            .filterDate(ee.Date(t0_str), ee.Date(t1_str))\n",
        "\n",
        "        # Aggregate the hourly images into a single daily image using the mean.\n",
        "        if aggregate == 'mean':\n",
        "            image = collection.mean()\n",
        "        elif aggregate == 'sum':\n",
        "            image = collection.sum()\n",
        "        elif aggregate == 'min':\n",
        "            image = collection.min()\n",
        "        elif aggregate == 'max':\n",
        "            image = collection.max()\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid aggregation method: {aggregate} can either be sum, min, max or mean\")\n",
        "\n",
        "        # Add bands containing the pixel longitude and latitude.\n",
        "        image = image.addBands(ee.Image.pixelLonLat())\n",
        "\n",
        "        # Use sampleRectangle to extract a grid of pixel values over the region.\n",
        "        region_data = image.sampleRectangle(region=region, defaultValue=0).getInfo()\n",
        "\n",
        "        # The pixel values for each band are in the \"properties\" dictionary.\n",
        "        props = region_data['properties']\n",
        "\n",
        "        # Extract the coordinate arrays from the added pixelLonLat bands.\n",
        "        lon_array = props['longitude']  # 2D array of longitudes\n",
        "        lat_array = props['latitude']   # 2D array of latitudes\n",
        "\n",
        "        # Determine the dimensions of the extracted grid.\n",
        "        nrows = len(lon_array)\n",
        "        ncols = len(lon_array[0]) if nrows > 0 else 0\n",
        "\n",
        "        # Identify the names of the bands that hold ERA5 variables, excluding the coordinate bands.\n",
        "        band_names = [key for key in props.keys() if key not in ['longitude', 'latitude']]\n",
        "\n",
        "        # Initialize results lists for each band if not already present.\n",
        "        for band in band_names:\n",
        "            if band not in results:\n",
        "                results[band] = []\n",
        "\n",
        "        # Loop over each pixel in the grid.\n",
        "        for i in range(nrows):\n",
        "            for j in range(ncols):\n",
        "                pixel_lon = lon_array[i][j]\n",
        "                pixel_lat = lat_array[i][j]\n",
        "                # For each ERA5 variable band, extract the pixel value and create a record.\n",
        "                for band in band_names:\n",
        "                    pixel_value = props[band][i][j]\n",
        "                    record = {\n",
        "                        'date': t0_str,  # daily timestamp as a string\n",
        "                        'latitude': pixel_lat,\n",
        "                        'longitude': pixel_lon,\n",
        "                        'value': pixel_value\n",
        "                    }\n",
        "                    results[band].append(record)\n",
        "\n",
        "        # Advance to the next day.\n",
        "        current = next_day\n",
        "\n",
        "    # Convert the accumulated results for each band into pandas DataFrames.\n",
        "    dataframes = {band: pd.DataFrame(records) for band, records in results.items()}\n",
        "    return dataframes\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xg_k50Z0EpWz"
      },
      "id": "xg_k50Z0EpWz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Extraction"
      ],
      "metadata": {
        "id": "BAxHyuqb4Z6H"
      },
      "id": "BAxHyuqb4Z6H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7213219a",
      "metadata": {
        "id": "7213219a",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Extract the TAHMO temperature 5 minute data for 2024 and extract the tmin, tavg and tmax\n",
        "# Load TAHMO EAC stations previously extracted\n",
        "eac_metadata = pd.read_csv(\"/content/drive/MyDrive/Datasets/ground/eac_stations.csv\")\n",
        "eac_metadata = eac_metadata[['code',\n",
        "                             'location.latitude',\n",
        "                             'location.longitude']].rename(columns={'location.latitude': 'lat',\n",
        "                                                                    'location.longitude': 'lon'})\n",
        "\n",
        "# Load the config file\n",
        "with open('/content/config (1).json', 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Set the api key and secret\n",
        "api_key = config['apiKey']\n",
        "api_secret = config['apiSecret']\n",
        "\n",
        "\n",
        "# Initialize the class\n",
        "rd = RetrieveData(apiKey=api_key,\n",
        "                  apiSecret=api_secret)\n",
        "\n",
        "\n",
        "# Get the temperature data for the EAC stations in 5min intervals\n",
        "eac_temp = rd.multiple_measurements(stations_list=eac_metadata['code'].tolist(),\n",
        "                                     startDate=start_date,\n",
        "                                     endDate=end_date,\n",
        "                                     variables=['te'],\n",
        "                                     csv_file = \"/content/drive/Shareddrives/NOAA-workshop/Datasets/ground/eac_temp_march_2024\",\n",
        "                                     aggregate='5min'\n",
        "                                     )\n",
        "\n",
        "\n",
        "# Aggregate the values to get the min, mean and max for the day\n",
        "tahmo_eac_tmin = rd.aggregate_variables(\n",
        "    eac_temp,\n",
        "    freq='1D',\n",
        "    method='min'\n",
        ")\n",
        "tahmo_eac_tavg = rd.aggregate_variables(\n",
        "    eac_temp,\n",
        "    freq='1D',\n",
        "    method='mean'\n",
        ")\n",
        "tahmo_eac_tmax = rd.aggregate_variables(\n",
        "    eac_temp,\n",
        "    freq='1D',\n",
        "    method='max'\n",
        ")\n",
        "\n",
        "\n",
        "plot_temperatures(tahmo_eac_tmin, tahmo_eac_tavg, tahmo_eac_tmax)\n",
        "\n",
        "\n",
        "# Save the variables\n",
        "tahmo_eac_tmin.to_csv(\"/content/drive/Shareddrives/NOAA-workshop/Datasets/ground/eac_tmin_march_2024.csv\", index=True)\n",
        "tahmo_eac_tavg.to_csv(\"/content/drive/Shareddrives/NOAA-workshop/Datasets/ground/eac_tavg_march_2024.csv\", index=True)\n",
        "tahmo_eac_tmax.to_csv(\"/content/drive/Shareddrives/NOAA-workshop/Datasets/ground/eac_tmax_march_2024.csv\", index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7df20e07",
      "metadata": {
        "id": "7df20e07"
      },
      "source": [
        "### GHCNd stations\n",
        "- Available globally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bdcaf02",
      "metadata": {
        "id": "0bdcaf02",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Extract GHCNd weather stations\n",
        "# @markdown They are distributed globally\n",
        "!git clone https://github.com/scotthosking/get-station-data.git\n",
        "\n",
        "import sys\n",
        "\n",
        "sys.path.append('get-station-data')\n",
        "\n",
        "from get_station_data import ghcnd\n",
        "from get_station_data.util import nearest_stn\n",
        "\n",
        "from utils.GHCN_stations import subset_stations_in_bbox, get_nearest_wmo_station, subset_noaa_stations_by_country, subset_weather_data_by_variable # GHCN station helper functions\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "@markdown Plot stations on a map\n",
        "import folium\n",
        "\n",
        "def plot_stations_folium(dataframes, colors=None, wmo=True):\n",
        "    \"\"\"\n",
        "    Plot stations from one or more dataframes on a Folium map.\n",
        "\n",
        "    Each dataframe must have 'location.latitude' and 'location.longitude' columns.\n",
        "    'colors' is a list specifying marker colors for each dataframe respectively.\n",
        "    \"\"\"\n",
        "    if colors is None:\n",
        "        colors = [\"blue\", \"red\", \"green\", \"purple\", \"orange\"]\n",
        "\n",
        "    # Create a base map; you can adjust the initial location/zoom as needed\n",
        "    m = folium.Map(location=[0, 0], zoom_start=2)\n",
        "\n",
        "    # Add markers for each dataframe\n",
        "    for df, color in zip(dataframes, colors):\n",
        "        for _, row in df.iterrows():\n",
        "          if wmo:\n",
        "              folium.Marker(\n",
        "                  location=[row[\"lat\"], row[\"lon\"]],\n",
        "                  tooltip=str(row[\"station\"]),   # <--- Pass the tooltip here\n",
        "                  icon=folium.Icon(color=color)\n",
        "              ).add_to(m)\n",
        "          else:\n",
        "              folium.Marker(\n",
        "                  location=[row[\"location.latitude\"], row[\"location.longitude\"]],\n",
        "                  tooltip=str(row[\"code\"]),   # <--- Pass the tooltip here\n",
        "                  icon=folium.Icon(color=color)\n",
        "              ).add_to(m)\n",
        "\n",
        "    return m\n",
        "\n",
        "\n",
        "stn_md = ghcnd.get_stn_metadata()\n",
        "# stn_md\n",
        "\n",
        "# Format the data rename lat and lon to latitude and longitude\n",
        "# stn_md = stn_md.rename(columns={'lat': 'latitude', 'lon': 'longitude'})\n",
        "\n",
        "\n",
        "wmo_ke_stations = subset_noaa_stations_by_country(stn_md, 'KE')\n",
        "wmo_ug_stations = subset_noaa_stations_by_country(stn_md, 'UG')\n",
        "wmo_rw_stations = subset_noaa_stations_by_country(stn_md, 'RW')\n",
        "\n",
        "# concatenate eac stations\n",
        "eac_wmo_stations = pd.concat([wmo_ke_stations, wmo_ug_stations, wmo_rw_stations])\n",
        "\n",
        "plot_stations_folium([wmo_ke_stations, wmo_ug_stations, wmo_rw_stations])\n",
        "\n",
        "# Get the data\n",
        "eac_wmo_data = ghcnd.get_data(eac_wmo_stations)\n",
        "\n",
        "eac_wmo_data[eac_wmo_data.station == 'KEM00063741']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualise WMO data nearest to you\n",
        "\n",
        "# Nairobi coordinates\n",
        "nairobi_coords = -1.293295, 37.822163\n",
        "kigali_coords = -1.944559, 30.061947\n",
        "kampala_coords = 0.315646, 32.581673\n",
        "\n",
        "''' feel free to add your own coords'''\n",
        "my_coords = ...\n",
        "\n",
        "# Get the nearest station\n",
        "nearest_stn(stn_md, nairobi_coords[1], nairobi_coords[0])\n",
        "\n",
        "\n",
        "# method given coordinates computes the nearest station together with the data\n",
        "def get_nearest_wmo_station_data(stn_md, eac_wmo_data, lat, lon):\n",
        "  import haversine as hs\n",
        "  # get the nearest data\n",
        "  nearest_station = nearest_stn(stn_md, lon, lat)\n",
        "\n",
        "  # compute the distance\n",
        "  # Extract scalar lat and lon values from the Series\n",
        "  nearest_lat = nearest_station['lat'].iloc[0]\n",
        "  nearest_lon = nearest_station['lon'].iloc[0]\n",
        "  distance = hs.haversine((lat, lon), (nearest_lat, nearest_lon))\n",
        "  print(nearest_station['station'].iloc[0])\n",
        "\n",
        "  # get the data from the station id\n",
        "  soi = eac_wmo_data[eac_wmo_data.station == nearest_station['station'].iloc[0]]\n",
        "\n",
        "  # Return the distance and the data\n",
        "  return distance, soi\n",
        "\n",
        "# write a method to extract tmin, tmax, tvg\n",
        "def extract_tmin_tmax_tvg(soi, plot=False):\n",
        "  tavg = subset_weather_data_by_variable(soi, 'TAVG', pivot=True)\n",
        "  tmin = subset_weather_data_by_variable(soi, 'TMIN', pivot=True)\n",
        "  tmax = subset_weather_data_by_variable(soi, 'TMAX', pivot=True)\n",
        "\n",
        "  if plot:\n",
        "    plot_temperatures(tmin, tmax, tavg)\n",
        "  else:\n",
        "    return tmin, tmax, tavg\n",
        "\n",
        "distance, soi = get_nearest_wmo_station_data(stn_md, eac_wmo_data, nairobi_coords[0], nairobi_coords[1])\n",
        "\n",
        "extract_tmin_tmax_tvg(soi, plot=True)\n",
        "\n",
        "eac_wmo_tmin, eac_wmo_tmax, eac_wmo_tavg = extract_tmin_tmax_tvg(eac_wmo_data)"
      ],
      "metadata": {
        "id": "Ub-biEHIHVsi",
        "cellView": "form"
      },
      "id": "Ub-biEHIHVsi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "67c26484",
      "metadata": {
        "id": "67c26484"
      },
      "source": [
        "#### Compare the temperature of TAHMO station/WMO station with the Met Agency data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a929878",
      "metadata": {
        "id": "3a929878"
      },
      "outputs": [],
      "source": [
        "# Compare the temperatures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de9b9bd1",
      "metadata": {
        "id": "de9b9bd1",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Extract CBAM temperature data (2018 -2024)\n",
        "# @markdown This data takes a while to extract and we shall be giving you the extracted data\n",
        "# Define parameters as shown in the docs\n",
        "product = 'cbam_historical_analysis'\n",
        "attribs = [\"max_temperature\", \"min_temperature\"]\n",
        "# start_date = '2012-01-01'\n",
        "# end_date = '2023-12-31'\n",
        "eac_bbox = [xmin, ymin, xmax, ymax]\n",
        "print(eac_bbox)\n",
        "\n",
        "# Create a client and extract data\n",
        "client = CBAMClient('config.json')\n",
        "cbam_data = extract_cbam_data(start_date='2012-01-01', end_date='2024-12-31',\n",
        "                         attributes=attribs, cbam_client=client, bbox=eac_bbox,\n",
        "                         output_type='netcdf')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ERA5 extract data from 1982 -2024\n",
        "\n",
        "\n",
        "\n",
        "import ee\n",
        "import io\n",
        "import os\n",
        "import tempfile\n",
        "import requests\n",
        "import datetime\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import rasterio\n",
        "from rasterio.io import MemoryFile\n",
        "from rasterio.transform import xy as rio_xy\n",
        "\n",
        "# Authenticate / initialize once (uncomment in interactive runtime)\n",
        "# ee.Authenticate()\n",
        "# ee.Initialize()\n",
        "\n",
        "def era5_yearly_to_inmemory_netcdf(\n",
        "    variable,\n",
        "    start_year=1982,\n",
        "    end_year=None,\n",
        "    region_ee_geometry=None,\n",
        "    dataset='ERA5_LAND',   # 'ERA5' or 'ERA5_LAND'\n",
        "    cadence='monthly',       # 'hourly' or 'daily' or 'monthly'\n",
        "    scale=None,            # meters (defaults used below)\n",
        "    crs='EPSG:4326',\n",
        "    save_local_copy=False, # also save .nc to local disk (path returned)\n",
        "    local_folder='./',\n",
        "    max_images_per_year=4000  # safety cutoff\n",
        "):\n",
        "    \"\"\"\n",
        "    For each year in [start_year, end_year], download the ERA5 images in that year,\n",
        "    aggregate them according to the specified cadence, build a time-x-y-xarray dataset\n",
        "    and write a NetCDF file for that year, then return the NetCDF as an in-memory\n",
        "    BytesIO object.\n",
        "\n",
        "    Returns:\n",
        "        dict: { year (int) : { 'nc_bytes': io.BytesIO, 'local_path': str or None } }\n",
        "    \"\"\"\n",
        "\n",
        "    if end_year is None:\n",
        "        end_year = datetime.datetime.utcnow().year\n",
        "\n",
        "    # Dataset selection and default scale (meters)\n",
        "    ds_upper = dataset.upper()\n",
        "    if ds_upper == 'ERA5_LAND' or ds_upper == 'ERA5-LAND' or ds_upper == 'ERA5LAND':\n",
        "        coll_hourly = 'ECMWF/ERA5_LAND/HOURLY'\n",
        "        coll_daily = 'ECMWF/ERA5_LAND/DAILY_AGGR'\n",
        "        default_scale = 11132\n",
        "    elif ds_upper == 'ERA5':\n",
        "        coll_hourly = 'ECMWF/ERA5/HOURLY'\n",
        "        coll_daily = 'ECMWF/ERA5/DAILY'\n",
        "        default_scale = 27830\n",
        "    else:\n",
        "        raise ValueError(\"dataset must be 'ERA5' or 'ERA5_LAND'\")\n",
        "\n",
        "    if scale is None:\n",
        "        scale = default_scale\n",
        "\n",
        "    if region_ee_geometry is None:\n",
        "        raise ValueError(\"region_ee_geometry (an ee.Geometry) is required (keep it small!)\")\n",
        "\n",
        "    # turn region into a geojson / coordinates object for getDownloadURL\n",
        "    # getInfo() here calls the server once\n",
        "    region_geojson = region_ee_geometry.getInfo()\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for year in range(start_year, end_year + 1):\n",
        "        print(f\"\\n--- Processing year {year} ---\")\n",
        "        start_date_year = f'{year}-01-01'\n",
        "        end_date_year = f'{year+1}-01-01'\n",
        "\n",
        "        if cadence == 'hourly':\n",
        "            coll = ee.ImageCollection(coll_hourly).filterDate(start_date_year, end_date_year).select(variable)\n",
        "        elif cadence == 'daily':\n",
        "            coll = ee.ImageCollection(coll_daily).filterDate(start_date_year, end_date_year).select(variable)\n",
        "        elif cadence == 'monthly':\n",
        "             # Process month by month for monthly aggregation\n",
        "            monthly_images = []\n",
        "            current_month_start = datetime.datetime.strptime(start_date_year, '%Y-%m-%d')\n",
        "            while current_month_start.year == year:\n",
        "                next_month_start = (current_month_start.replace(day=1) + datetime.timedelta(days=32)).replace(day=1)\n",
        "                coll_hourly_month = ee.ImageCollection(coll_hourly).filterDate(current_month_start, next_month_start).select(variable)\n",
        "                monthly_image = coll_hourly_month.mean() # Aggregate hourly to monthly mean\n",
        "                monthly_images.append(monthly_image.set('system:time_start', ee.Date(current_month_start)))\n",
        "                current_month_start = next_month_start\n",
        "            coll = ee.ImageCollection(monthly_images)\n",
        "        else:\n",
        "            raise ValueError(\"cadence must be 'hourly', 'daily', or 'monthly'\")\n",
        "\n",
        "\n",
        "        try:\n",
        "            n_images = int(coll.size().getInfo())\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Could not fetch collection size for {year}: {e}\")\n",
        "\n",
        "        if n_images == 0:\n",
        "            print(f\"No images found for {year} (variable '{variable}', cadence '{cadence}'). Skipping.\")\n",
        "            continue\n",
        "\n",
        "        if n_images > max_images_per_year and cadence != 'monthly': # Allow more images for monthly aggregation\n",
        "             raise RuntimeError(f\"Year {year} has {n_images} images > max_images_per_year ({max_images_per_year}). Aborting for safety.\")\n",
        "\n",
        "        print(f\"Found {n_images} images for {year}. Downloading each to memory (this may be slow).\")\n",
        "\n",
        "        # Build lists to stack\n",
        "        img_arrays = []\n",
        "        times = []\n",
        "        ref_shape = None\n",
        "        ref_transform = None\n",
        "        ref_crs = None\n",
        "\n",
        "        # Convert collection to server list and iterate\n",
        "        coll_list = coll.toList(n_images)\n",
        "\n",
        "        for i in range(n_images):\n",
        "            ee_img = ee.Image(coll_list.get(i))\n",
        "            # time string\n",
        "            try:\n",
        "                time_start_ms = ee.Date(ee_img.get('system:time_start')).getInfo()['value']\n",
        "                time_str = datetime.datetime.fromtimestamp(time_start_ms / 1000.0).strftime('%Y-%m-%d')\n",
        "            except Exception:\n",
        "                # fallback: use index-based date\n",
        "                time_str = f'{year}-unknown-{i}'\n",
        "            print(f\"  - image {i+1}/{n_images} date {time_str} ...\", end=' ', flush=True)\n",
        "\n",
        "            # Request a GeoTIFF download URL (format GEO_TIFF to get raw .tif bytes)\n",
        "            params = {\n",
        "                'bands': [variable],\n",
        "                'region': region_geojson,   # geojson-like mapping or coordinates (small)\n",
        "                'scale': int(scale),\n",
        "                'format': 'GEO_TIFF',\n",
        "                'filePerBand': False\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                url = ee_img.getDownloadURL(params)\n",
        "            except Exception as e:\n",
        "                raise RuntimeError(f\"getDownloadURL failed for {year} image idx {i}: {e}\")\n",
        "\n",
        "            # Download bytes (may be zipped or raw GeoTIFF depending on params; we asked GEO_TIFF)\n",
        "            r = requests.get(url, timeout=600)\n",
        "            if r.status_code != 200:\n",
        "                raise RuntimeError(f\"HTTP error {r.status_code} when downloading image: {r.text[:200]}\")\n",
        "\n",
        "            # Load into rasterio MemoryFile\n",
        "            with MemoryFile(r.content) as mem:\n",
        "                with mem.open() as src:\n",
        "                    arr = src.read(1)           # single-band image\n",
        "                    transform = src.transform\n",
        "                    crs_src = src.crs\n",
        "                    h, w = src.height, src.width\n",
        "\n",
        "            # check shape consistency\n",
        "            if ref_shape is None:\n",
        "                ref_shape = (h, w)\n",
        "                ref_transform = transform\n",
        "                ref_crs = crs_src\n",
        "            else:\n",
        "                if (h, w) != ref_shape:\n",
        "                    raise RuntimeError(f\"Image {i} shape {h,w} differs from first image shape {ref_shape}. Reprojection/resampling not implemented - aborting.\")\n",
        "\n",
        "            img_arrays.append(arr)\n",
        "            times.append(np.datetime64(time_str))\n",
        "            print(\"OK\")\n",
        "\n",
        "        # Stack into ndarray (time, y, x)\n",
        "        data_stack = np.stack(img_arrays, axis=0)  # shape (time, H, W)\n",
        "        print(f\"Stacked data: {data_stack.shape}\")\n",
        "\n",
        "        # Build coordinate vectors from transform\n",
        "        height, width = ref_shape\n",
        "        # x coords (cols)\n",
        "        xs = np.array([rio_xy(ref_transform, 0, col, offset='center')[0] for col in range(width)])\n",
        "        # y coords (rows) - note rasterio returns y per (row, col); rows increase downward\n",
        "        ys = np.array([rio_xy(ref_transform, row, 0, offset='center')[1] for row in range(height)])\n",
        "\n",
        "        # xarray DataArray\n",
        "        da = xr.DataArray(\n",
        "            data_stack,\n",
        "            dims=('time', 'y', 'x'),\n",
        "            coords={'time': times, 'y': ys, 'x': xs},\n",
        "            name=variable\n",
        "        )\n",
        "\n",
        "        ds = xr.Dataset({variable: da})\n",
        "        ds.attrs['source'] = f\"GEE {coll_daily} ({dataset})\" # Note: still using daily collection ID in source attr\n",
        "        # Convert region_geojson to a string for NetCDF compatibility\n",
        "        ds.attrs['region'] = json.dumps(region_geojson)\n",
        "        ds.attrs['scale_m'] = scale\n",
        "\n",
        "        # Persist to a temporary netCDF file, then load bytes into memory\n",
        "        tmpf = tempfile.NamedTemporaryFile(suffix=f\"_{variable}_{year}_{cadence}.nc\", delete=False)\n",
        "        tmpf.close()\n",
        "        try:\n",
        "            ds.to_netcdf(tmpf.name, engine='netcdf4')\n",
        "        except Exception as e:\n",
        "            os.unlink(tmpf.name)\n",
        "            raise RuntimeError(f\"Failed to write NetCDF for year {year}: {e}\")\n",
        "\n",
        "        # Read bytes into memory BytesIO\n",
        "        with open(tmpf.name, 'rb') as f:\n",
        "            nc_bytes = f.read()\n",
        "\n",
        "        # Optionally save a local persistent copy\n",
        "        local_path = None\n",
        "        if save_local_copy:\n",
        "            os.makedirs(local_folder, exist_ok=True)\n",
        "            local_path = os.path.join(local_folder, f\"{variable}_{year}_{cadence}.nc\")\n",
        "            with open(local_path, 'wb') as f:\n",
        "                f.write(nc_bytes)\n",
        "\n",
        "        # Cleanup temp file\n",
        "        os.unlink(tmpf.name)\n",
        "\n",
        "        results[year] = {'nc_bytes': io.BytesIO(nc_bytes), 'local_path': local_path}\n",
        "\n",
        "        print(f\"Year {year} done: NetCDF in memory ({len(nc_bytes)/1e6:.2f} MB).\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "roi = ee.Geometry.Polygon(eac_region)\n",
        "\n",
        "out = era5_yearly_to_inmemory_netcdf(\n",
        "    variable='temperature_2m',\n",
        "    start_year=1982,\n",
        "    end_year=2024,\n",
        "    region_ee_geometry=roi,\n",
        "    dataset='ERA5',        # or 'ERA5_LAND'\n",
        "    cadence='monthly',\n",
        "    scale=27830,           # use native-ish scale for ERA5 (meters)\n",
        "    save_local_copy=False\n",
        ")\n",
        "\n",
        "# Access the NetCDF bytes for 2023:\n",
        "nc_bytesio = out[2023]['nc_bytes']      # io.BytesIO\n",
        "# To load into xarray directly from memory:\n",
        "nc_bytesio.seek(0)\n",
        "ds = xr.open_dataset(nc_bytesio)\n",
        "print(ds)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qYQLdRZ2Nnhd"
      },
      "id": "qYQLdRZ2Nnhd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d6fcb3aa",
      "metadata": {
        "id": "d6fcb3aa"
      },
      "source": [
        "## Data Processing\n",
        "\n",
        "1. Ground (GHCN/TAHMO/your station) vs CBAM/ERA5\n",
        "2. PET comparison plot (CBAM vs ERA5)\n",
        "3. Count of heat/agri stress days --- Plot of the heat exchange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a85bc980",
      "metadata": {
        "id": "a85bc980",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Load TAHMO temperature data\n",
        "# Load the tmin, tmax, tavg files\n",
        "tahmo_eac_tmin = pd.read_csv(\"/content/drive/Shareddrives/NOAA-workshop/Datasets/ground/eac_tmin_march_2024.csv\", index_col=0)\n",
        "tahmo_eac_tmax = pd.read_csv(\"/content/drive/Shareddrives/NOAA-workshop/Datasets/ground/eac_tmax_march_2024.csv\", index_col=0)\n",
        "tahmo_eac_tavg = pd.read_csv(\"/content/drive/Shareddrives/NOAA-workshop/Datasets/ground/eac_tavg_march_2024.csv\", index_col=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load ERA5 Data Monthly\n",
        "era5_march = xr.open_dataset(\"/content/drive/Shareddrives/NOAA-workshop/Datasets/reanalysis/era5/era5_temperature_agg_march_2024.nc\")\n",
        "\n",
        "# rename lat and lon to x and y\n",
        "era5_march = era5_march.rename({'lat': 'y', 'lon': 'x'})\n",
        "\n",
        "era5_march\n",
        "\n",
        "plot_xarray_data2(\n",
        "    xarray_ds=era5_march,\n",
        "    fig_title=\"Temperature over EAC (KE, RW, UG) Region ERA5\",\n",
        "    columns=[\"min_temperature\", \"avg_temperature\", \"max_temperature\"],\n",
        "    plot_size=4,\n",
        "    bbox=[xmin, ymin, xmax, ymax],\n",
        "    save=True\n",
        ")"
      ],
      "metadata": {
        "id": "tCRMAsHgPUb5",
        "cellView": "form"
      },
      "id": "tCRMAsHgPUb5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load ERA5 1982-2024\n",
        "\n",
        "era5_monthly = xr.open_dataset(\"/content/drive/Shareddrives/NOAA-workshop/Datasets/reanalysis/era5/era5_temperature_monthly_1982_2024_combined.nc\")\n",
        "# rename x and y to latitude and longitude\n",
        "era5_monthly = era5_monthly.rename({'x': 'longitude', 'y': 'latitude'})\n",
        "\n",
        "# Subtract 273.15 to convert from Kelvin to degrees celsius\n",
        "era5_monthly['temperature_2m'] = era5_monthly['temperature_2m'] - 273.15\n",
        "\n",
        "# Convert time to datetime objects\n",
        "era5_monthly['time'] = pd.to_datetime(era5_monthly['time'].values)\n",
        "\n",
        "plot_xarray_data2(\n",
        "    xarray_ds=era5_monthly,\n",
        "    fig_title=\"ERA5 Temperature Analysis in East Africa\",\n",
        "    columns=[\"temperature_2m\"],\n",
        "    plot_size=7,\n",
        "    bbox=[xmin, ymin, xmax, ymax],\n",
        ")"
      ],
      "metadata": {
        "id": "TBKLnLDfJJrg",
        "cellView": "form"
      },
      "id": "TBKLnLDfJJrg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load CBAM Data\n",
        "# Data from 2018-2024\n",
        "cbam_eac = xr.open_dataset('/content/drive/Shareddrives/NOAA-workshop/Datasets/reanalysis/CBAM_temp2018_2024.nc')\n",
        "\n",
        "# Subset for march 2024\n",
        "# cbam_eac = cbam_eac.sel(date=slice('2024-03-01', '2024-03-31'))\n",
        "\n",
        "# # Agreegate the data from daiy to monthly\n",
        "# cbam_eac_monthly = cbam_eac.resample(time='M').mean()\n",
        "\n",
        "# cbam_eac_monthly\n",
        "\n",
        "del cbam_eac\n",
        "\n",
        "# select the month of march 2024\n",
        "cbam_eac_march = cbam_eac.sel(date=slice('2024-03-01', '2024-03-31'))\n",
        "# compute the avg_temperature from the min and max temperature by computing the sum and dividing by 2\n",
        "cbam_eac_march['avg_temperature'] = (cbam_eac_march['max_temperature'] + cbam_eac_march['min_temperature']) / 2"
      ],
      "metadata": {
        "id": "R-HUbWhGLuKR",
        "cellView": "form"
      },
      "id": "R-HUbWhGLuKR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ERA5 and CBAM Comparison\n",
        "# rename date to time\n",
        "cbam_eac_march = cbam_eac_march.rename({'date': 'time'})\n",
        "# rename cbam lat lon to x y\n",
        "cbam_eac_march = cbam_eac_march.rename({'lat': 'y', 'lon': 'x'})\n",
        "\n",
        "datasets = [era5_march['max_temperature'].to_dataset(), cbam_eac_march['max_temperature'].to_dataset()]\n",
        "\n",
        "compare_xarray_datasets2(\n",
        "    datasets,\n",
        "    labels=['max_temperature ERA5', 'max_temperature CBAM'],\n",
        "    fig_title='Temp comparison',\n",
        "    bboxes=[[xmin, ymin, xmax, ymax], [xmin, ymin, xmax, ymax]],\n",
        "    save=True\n",
        ")"
      ],
      "metadata": {
        "id": "VlozkbbzXE1Y",
        "cellView": "form"
      },
      "id": "VlozkbbzXE1Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PET Comparison\n",
        "# @markdown Potential Evapotranspiration\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def pet_hargreaves(tmin, tmax, tmean, Ra=15.0):\n",
        "    dtr = np.maximum(tmax - tmin, 0)\n",
        "    return 0.0023 * Ra * (tmean + 17.8) * np.sqrt(dtr)\n",
        "\n",
        "def rmse(a,b): return float(np.sqrt(np.nanmean((np.asarray(a)-np.asarray(b))**2)))\n",
        "\n",
        "pet_cbam = pet_hargreaves(cbam_eac_march['min_temperature'], cbam_eac_march['max_temperature'], cbam_eac_march['avg_temperature'])\n",
        "# Assign a name to the DataArray before converting to Dataset\n",
        "pet_cbam = pet_cbam.to_dataset(name='pet')\n",
        "\n",
        "pet_era5 = pet_hargreaves(era5_march['min_temperature'], era5_march['max_temperature'], era5_march['avg_temperature'])\n",
        "pet_era5 = pet_era5.to_dataset(name='pet')\n",
        "\n",
        "# Compare PET from ERA5 and CBAM\n",
        "compare_xarray_datasets2(\n",
        "    [pet_era5, pet_cbam],\n",
        "    labels=['PET ERA5', 'PET CBAM'],\n",
        "    fig_title='PET Comparison (ERA5 vs CBAM) - March 2024',\n",
        "    bboxes=[[xmin, ymin, xmax, ymax], [xmin, ymin, xmax, ymax]],\n",
        "    save=True\n",
        ")"
      ],
      "metadata": {
        "id": "MNCPDCJhXY1q",
        "cellView": "form"
      },
      "id": "MNCPDCJhXY1q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Computing stress\n",
        "stress_cbam = (pet_cbam>5) &(cbam_eac_march['max_temperature']>32)\n",
        "stress_era5 = (pet_era5>5) &(era5_march['max_temperature']>32)\n",
        "\n",
        "# get the stress days\n",
        "print(\"Heat/Agri stress days (CBAM): \", )"
      ],
      "metadata": {
        "id": "UeWF0-Vm93NO",
        "cellView": "form"
      },
      "id": "UeWF0-Vm93NO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Examining the Heat change over the years\n",
        "\n",
        "1. Use WMO Stations and get the overall heat in the nearest station\n",
        "2. Look at all the stations maximum temperature reached over the years as a heatmap\n",
        "3. Set your own coordinates for your location in East Africa (KE, UG, RW) to get the data\n",
        "4. Get the monthly ERA5 data and visualise a similar heatmap\n",
        "5. Visualise the heat change comparison of the 2"
      ],
      "metadata": {
        "id": "yPaPm6vNIYgu"
      },
      "id": "yPaPm6vNIYgu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "558f87d4",
        "cellView": "form"
      },
      "source": [
        "# method to create a heatmap of maximum temperature for WMO stations\n",
        "def plot_wmo_heatmap(annual_max_temp_stations, fig_title=\"Annual Maximum Temperature Heatmap (WMO Stations)\"):\n",
        "    \"\"\"\n",
        "    Plots a heatmap of annual maximum temperature for WMO stations.\n",
        "\n",
        "    Args:\n",
        "        annual_max_temp_stations (pd.DataFrame): DataFrame with annual maximum temperatures per station.\n",
        "        fig_title (str): Title of the heatmap.\n",
        "    \"\"\"\n",
        "    # localise to None date\n",
        "    # annual_max_temp_stations = annual_max_temp_stations.T\n",
        "    annual_max_temp_stations.index = pd.to_datetime(annual_max_temp_stations.index)\n",
        "    annual_max_temp_stations = annual_max_temp_stations.tz_localize(None)\n",
        "    annual_max_temp_stations = annual_max_temp_stations.T\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    sns.heatmap(annual_max_temp_stations,\n",
        "                cmap='YlOrRd',\n",
        "                cbar_kws={'label': 'Maximum Temperature (¬∞C)'}) # Fixed cbar_label\n",
        "    plt.title(fig_title)\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Station')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# method to create a heatmap of temperature for ERA5 monthly data\n",
        "def plot_era5_heatmap(era5_monthly_data, variable='temperature_2m', fig_title=\"Monthly Temperature Heatmap (ERA5)\"):\n",
        "    \"\"\"\n",
        "    Plots a heatmap of monthly temperature for ERA5 data.\n",
        "\n",
        "    Args:\n",
        "        era5_monthly_data (xr.Dataset): Xarray Dataset containing monthly ERA5 temperature data.\n",
        "        variable (str): The variable to plot from the ERA5 dataset.\n",
        "        fig_title (str): Title of the heatmap.\n",
        "    \"\"\"\n",
        "    # Assuming the ERA5 data has dimensions 'time', 'latitude', 'longitude'\n",
        "    monthly_mean_temp = era5_monthly_data[variable].mean(dim=['latitude', 'longitude'])\n",
        "\n",
        "    # Convert to pandas Series for plotting\n",
        "    monthly_mean_temp_series = monthly_mean_temp.to_series()\n",
        "\n",
        "    # Create a MultiIndex from the time index for unstacking\n",
        "    monthly_mean_temp_series.index = pd.MultiIndex.from_arrays([\n",
        "        monthly_mean_temp_series.index.year,\n",
        "        monthly_mean_temp_series.index.month\n",
        "    ], names=['year', 'month'])\n",
        "\n",
        "\n",
        "    # Reshape for heatmap (Month x Year)\n",
        "    heatmap_data = monthly_mean_temp_series.unstack().T\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    sns.heatmap(heatmap_data, cmap='YlOrRd', cbar_kws={'label': 'Mean Temperature (¬∞C)'}) # Fixed cbar_label\n",
        "    plt.title(fig_title)\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Month')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# method to visualize heat change comparison (this will depend on the specific comparison needed)\n",
        "def compare_heatmaps(heatmap1_data, heatmap2_data, label1, label2, fig_title=\"Heatmap Comparison\"):\n",
        "    \"\"\"\n",
        "    Visualizes the comparison of two heatmaps.\n",
        "\n",
        "    Args:\n",
        "        heatmap1_data (pd.DataFrame): Data for the first heatmap.\n",
        "        heatmap2_data (pd.DataFrame): Data for the second heatmap.\n",
        "        label1 (str): Label for the first heatmap.\n",
        "        label2 (str): Label for the second heatmap.\n",
        "        fig_title (str): Title for the comparison plot.\n",
        "    \"\"\"\n",
        "    # This is a placeholder. Actual comparison visualization will depend on the data structure and desired comparison.\n",
        "    print(f\"Comparison visualization between {label1} and {label2} needs to be implemented based on the specific comparison method (e.g., difference, correlation).\")\n",
        "    # Example: Plotting the difference (requires both heatmaps to have the same structure)\n",
        "    # if heatmap1_data.shape == heatmap2_data.shape:\n",
        "    #     difference_heatmap = heatmap1_data - heatmap2_data\n",
        "    #     plt.figure(figsize=(15, 8))\n",
        "    #     sns.heatmap(difference_heatmap, cmap='coolwarm', center=0, cbar_label='Temperature Difference (¬∞C)')\n",
        "    #     plt.title(f'{fig_title} (Difference: {label1} - {label2})')\n",
        "    #     plt.xlabel('Year')\n",
        "    #     plt.ylabel('Month/Station') # Adjust label based on heatmap structure\n",
        "    #     plt.tight_layout()\n",
        "    #     plt.show()\n",
        "    # else:\n",
        "    #     print(\"Heatmaps have different shapes and cannot be directly subtracted for difference visualization.\")\n",
        "\n",
        "# Set your own coordinates (example using Nairobi coordinates)\n",
        "my_lat = nairobi_coords[0]\n",
        "my_lon = nairobi_coords[1]\n",
        "\n",
        "# Use WMO Stations and get the overall heat in the nearest station\n",
        "distance, nearest_wmo_data = get_nearest_wmo_station_data(stn_md, eac_wmo_data, my_lat, my_lon)\n",
        "\n",
        "# Extract TMIN, TMAX, TAVG for the nearest station\n",
        "nearest_wmo_tmin, nearest_wmo_tmax, nearest_wmo_tavg = extract_tmin_tmax_tvg(nearest_wmo_data)\n",
        "\n",
        "print(\"\\nTemperature data for the nearest WMO station:\")\n",
        "display(nearest_wmo_tavg.head())\n",
        "\n",
        "\n",
        "# Visualise the heat change comparison of the 2\n",
        "# For comparison, let's use the mean annual maximum temperature from WMO stations\n",
        "# and the mean annual temperature from ERA5\n",
        "wmo_mean_annual_max = annual_max_temp_stations.mean(axis=1).to_frame(name='WMO Mean Annual Max Temp')\n",
        "\n",
        "# For ERA5, let's calculate the mean annual temperature over the region\n",
        "era5_mean_annual = era5_monthly['temperature_2m'].mean(dim=['latitude', 'longitude']).resample(time='Y').mean().to_series().to_frame(name='ERA5 Mean Annual Temp')\n",
        "\n",
        "# Align the dataframes by year\n",
        "comparison_df = pd.concat([wmo_mean_annual_max, era5_mean_annual], axis=1)\n",
        "\n",
        "\n",
        "# Look at all the stations maximum temperature reached over the years as a heatmap\n",
        "# Ensure annual_max_temp_stations is calculated (if not already)\n",
        "if 'annual_max_temp_stations' not in locals():\n",
        "    annual_max_temp_stations = eac_wmo_tmax.resample('Y').max()\n",
        "\n",
        "plot_wmo_heatmap(annual_max_temp_stations)\n",
        "# Plot the comparison\n",
        "plt.figure(figsize=(12, 6))\n",
        "comparison_df.plot(ax=plt.gca())\n",
        "plt.title('Mean Annual Temperature Comparison (WMO vs ERA5)')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Temperature (¬∞C)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Call the compare_heatmaps function (placeholder)\n",
        "# compare_heatmaps(annual_max_temp_stations, heatmap_data, 'WMO Annual Max Temp', 'ERA5 Monthly Mean Temp')\n",
        "\n",
        "# @title Plot ERA5 Heatmap over time\n",
        "# Ensure era5_monthly is loaded and processed (if not already)\n",
        "if 'era5_monthly' not in locals():\n",
        "    era5_monthly = xr.open_dataset(\"/content/drive/Shareddrives/NOAA-workshop/Datasets/reanalysis/era5/era5_temperature_monthly_1982_2024_combined.nc\")\n",
        "    era5_monthly = era5_monthly.rename({'x': 'longitude', 'y': 'latitude'})\n",
        "    era5_monthly['temperature_2m'] = era5_monthly['temperature_2m'] - 273.15\n",
        "    era5_monthly['time'] = era5_monthly['time'].dt.strftime('%Y-%m')\n",
        "\n",
        "plot_era5_heatmap(era5_monthly, variable='temperature_2m', fig_title=\"Monthly Mean Temperature Heatmap (ERA5)\")"
      ],
      "id": "558f87d4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89a8cfa8",
        "cellView": "form"
      },
      "source": [
        "# @title Plot WMO Heatmap\n",
        "# Ensure annual_max_temp_stations is calculated (if not already)\n",
        "if 'annual_max_temp_stations' not in locals():\n",
        "    annual_max_temp_stations = eac_wmo_tmax.resample('Y').mean()\n",
        "\n",
        "plot_wmo_heatmap(annual_max_temp_stations, 'Annual Average Maximum Temperature Heatmap (WMO Stations)')"
      ],
      "id": "89a8cfa8",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}